# VPN(Virtual Private Network)

VPN은 프로그래밍 뿐만 아니라 일상 생활에서도 많이 사용한다(IP를 바꾸거나, 회사 네트워크에 접속).

# 1. PN

VPN의 핵심은 Virtual(가상)이다. VPN에 대응하는 기념은 PN(사설망)이다.

인터넷은 누구나 이용하기 때문에 공개된 길(광랜이나 집, 회사로 들어가는 UTP 선)로 데이터가 흐른다. HTTPS같은 것으로 보호는 하지만 결국 공개는 돼 있다.

보안에 위협이 되기 때문에 회사에서는 공개된 인터넷 망이 아닌 자기만 사용하는 망을 원할 수 있다. 하지만 이것은 물리적으로 큰 비용이 든다. 사내나 같은 계열사인 회사가 가깝다면 정말 사설망을 깔수도 있지만 만약 사원들이 재택을 한다면??

# 2. VPN

이때 공개된 인터넷 망과 소프트웨어(프로그램, 애플리케이션)를 통해 사설망과 비슷한 효과를 준다. 그게 가상의 사설망 VPN이다. 사설망과 똑같이 감시와 필터링을 할수 있다(즉 재택을 해서 회사 VPN으로 접속을 한다면 회사에서 데이터를 열람할 수 있다).

# Proxy, Gateway

보통 인터넷 데이터의 흐름은 클라이언트 ↔ 인터넷 ↔ 서버로 이뤄진다. 만약 여기서 서버가 클라이언트의 서버(개발 환경)이라면 인터넷 없이 스위치나 라우터로도 충분하다.

# 1. 프록시

클라이언트와 서버 사이에 위치해있다. 주로 클라이언트에서 인터넷으로 가기 직전과 인터넷에서 서버로 가기 직전에 프록시가 있다.(클라이언트 ↔ 프록시 ↔ 인터넷 ↔ 프록시 ↔ 서버)

이때 클라이언트 쪽에 붙어 있는 프록시를 포워드 프록시, 서버쪽에 붙어있는 프록시를 리버스 프록시라 부른다.

## 1.1. 포워드 프록시

클라이언트는 왜 프록시를 거쳐 요청을 보낼까??

사실 포워드 프록시든 리버스 프록시든 기능은 만들기 나름이다.

보통 요청을 할 때 브라우저가 사용자 ip나 기본적인 헤더를 직접 넣어서 보내버린다. 이때 클라이언트 서버에 포워드 프록시를 넣어 브라우저에서 강제로 넣는 값들을 필터링하여 제거 가능하다.

또한 클라이언트에서 서버로 요청을 보낼 때, 정말 요청이 가도 되는지 한번 더 검열, 점검, 필터링 해주는 역할을 한다.

## 1.2. 리버스 프록시

불필요한 요청이 왓을 경우 필터링한다. 유명한 프록시 서버로 Apache, Nginx 같은 것들이 있다.

리버스 프록시 또한 가장 중요한 역할은 필터링 해주는 것이다.

추가적으로 HTTPS를 적용해주는 역할도 하고, 정적 파일(HTML, CSS, JS, image, font)을 서빙하는 역할도 한다. 즉 요청이 원서버 까지 안가더라도 프록시에서 제공(캐시)해준다. Gzip이나 Brotli로 압축도 해준다.

마지막으로 로드 밸런싱과 오토 스케일링 역할도 한다. 로드 밸런싱은 서버가 여러대일 때 요청 별로 어떤 서버로 보내줄지 분배한다면, 오토 스케일링은 요청이 너무 많아질 때 서버를 늘려주는 역할은 한다.

그럼 왜 원서버로 요청을 보내지 않고 프록시를 거칠까?? 이유는 원 서버보다 프록시가 더 빠르기 때문이다.

# 2. 게이트웨이

프록시와 비슷한 개념으로 프록시 자리에 게이트웨이가 대신 들어갈 수도 있다. 즉 경계는 있으나 경계선이 모호해졌다. 주 목적은 프로토콜을 바꾸는 것이다.
(클라이언트 → 인터넷(HTTP) → 게이트웨이(HTTPS/HTTP,WS,FTS) → 서버)

## 2.1. 프록시와 게이트웨이의 구별이 유명무실해진 이유

프록시에서도 HTTPS를 적용 가능해졌다.

리버스 프록시에 HTTPS를 적용하면 클라이언트가 HTTPS 요청을 보냈을 때 프록시가 HTTP로 바꿔서 서버 그룹 안에서는 HTTP 요청을 한다. 이렇게 하는 이유는 같은 서버 내부에서 암호화를 하고 복호화를 하면 자원이 들기 때문에 공개된 통로인 인터넷을 왔다갔다 할때만 암호화를 한다.

리버스 프록시에서도 HTTPS를 관장하여 인터넷에서 들어오는 HTTPS를 HTTP로 복호화하여 진짜 서버에 보낼수 있다. 리버스 프록시는 게이트웨이 역할을 할수 있는데 더 다양한 역할도 소화 가능하다. 즉 게이트웨이만 따로 쓸 필요가 없는 상황이고 이로 인해 프록시와 게이트웨이의 구별을 엄격하게 하지도 않는다.

### 2.1.1. 예시 API Gateway

AWS API Gateway와 GCP API Gateway 또한 이름은 게이트웨이이지만 실제는 프록시이다.

## 2.2. HTTP Method CONNECT

클라이언트에서 진짜 서버로 요청을 이 요청을 보낸다. 이때 중간에 프록시나 게이트웨이를 만나도 요청을 바꾸지 않고 서버로 그대로 요청을 하겠단 뜻이다.

클라이언트가 서버로부터 직접적으로 요청을 하는 것이기 때문에, 중간에 프록시가 거부할수도 있다. 다만 인증된 사람일 경우 가능할수도 있다.

# 8. 통합점: 게이트웨이, 터널, 릴레이

- 게이트웨이: 서로 다른 프로토콜과 애플리케이션 간의 HTTP 인터페이스
- 애플리케이션 인터페이스: 서로 다른 형식의 웹 애플리케이션이 통신하는 데 사용
- 터널: HTTP 커넥션을 통해서 HTTP가 아닌 트래픽을 전송하는 데 사용
- 릴레이: 일종의 단순한 HTTP 프락시로, 한 번에 한 개의 홉에 데이터를 전달하는 데 사용

# 8.1. 게이트웨이

HTTP의 확장과 인터페이스는 필요에 따라 발전했다. 웹에서도 HTTP 외에 다른 프로토콜을 사용하는 애플리케이션을 연결하고 싶었는데, 이 때 게이트웨이가 타 프로토콜을 HTTP에 연결하는 역할을 한다.

게이트웨이는 요청을 받고 응답을 받는 포털처럼 동작하며, 동적인 콘텐츠를 생성하거나 데이터 베이스에 질의를 보낼 수 있다.

![스크린샷 2023-05-26 오후 1.24.09.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/39a7991f-b8f6-45c7-8b57-cfd9f5ed85e4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.24.09.png)

- (a): 게이트웨이는 FTP URL을 가리키는 HTTP 요청을 받는다. 게이트 웨이는 FTP 커넥션을 맺고 적절한 요청을 보낸다. 게이트 웨이는 FTP로부터 받은 응답을 적절한 HTTP 헤더와 함께 HTTP를 통해 클라이언트로 보낸다.
- (b): 게이트웨이는 암호화된 요청을 SSL를 통해 받고, 요청을 해석하여 HTTP 요청을 원서버에 전달한다.
- (c): 게이트웨이는 애플리케이션 서버 게이트웨이 API를 통해 서버측의 프로그램에 연결된다.

## 8.1.1. 클라이언트 측 게이트웨이와 서버 측 게이트웨이

웹 게이트웨이는 한쪽은 HTTP로 다른 한쪽은 다른 프로토콜로 통신하는데 빗금(/)으로 구분한다. [클라이언트 측 프로토콜]/[서버 측 프로토콜](ex. HTTP/NNTP 는 HTTP로 접근한 클라이언트가 NNTP를 사용하는 서버와 게이트웨이로 연결한 것)

<aside>
💡 상이한 HTTP 버전 사이에서 변환을 수행하는 웹 프락시는 게이트웨이와 같다. 그 웹 프락시는 양쪽 사이에서 교섭을 위한 복잡한 로직을 수행하기 때문이다. 하지만 양쪽에서 HTTP로 통신하기 때문에, 기술적으로는 프락시다.

</aside>

# 8.2. 프로토콜 게이트웨이

아래 그림처럼 클라이언트에서 FTP를 처리할 게이트웨이 설정을 한다.

![스크린샷 2023-05-26 오후 1.35.30.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/73229fb5-fc2f-4303-b4c0-01a3fb45d273/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.35.30.png)

이렇게하면 일반적인 HTTP 요청을 원서버로 가지만, FTP 요청이 들어있는 HTTP 요청은 HTTP/FTP 게이트웨이에서 처리된 후 바로 클라이언트에게 응답한다.

![스크린샷 2023-05-26 오후 1.37.04.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1a6634cb-af3d-4056-bbc9-6743600e2d68/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.37.04.png)

## 8.2.1. HTTP/*: 서버 측 웹 게이트웨이

HTTP/FTP 기준 게이트웨이에서 FTP 프로토콜에 대한 요청 처리는 아래와 같다.

![스크린샷 2023-05-26 오후 1.51.55.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ca36cea3-abce-4790-87f9-16c56e9b6c75/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.51.55.png)

- USER와 PASS 명령을 보내서 서버에 로그인한다.
- 서버에서 적절한 디렉터리로 변경하기 위해 CWD 명령을 보낸다.
- 다운로드 형식을 ASCII로 설정한다.
- MDTM으로 문서의 최근 수정 시간을 가져온다.
- PASV로 서버에게 수동형 데이터 검색을 하겠다고 말한다.
- TETR로 객체를 검색한다.
- 제어 채널에서 반환된 포트로 FTP 서버에 데이터 커넥션을 맺는다. 데이터 채널이 열리는 대로, 객체가 게이트웨이로 전송된다.

## 8.2.2. HTTP/HTTPS: 서버 측 보안 게이트웨이

보안을 제공하는 방식으로 HTTPS 프로토콜만 사용하는 서버가 있을 때, 클라이언트 측에서 HTTP로 요청을 보내면 게이트웨이에서 사용자의 모든 세션을 암호화 하여 통신을 하도록 한다.

## 8.2.3. HTTPS/HTTP: 클라이언트 축 보안 가속 게이트웨이

어떤 내부망이 있고, 내부 망은 보안 처리가 되어 있어서 외부에서는 접근이 되지 않지만, 내부망에서는 HTTP 프로토콜을 사용한다고 하자. 그때 보안이 HTTPS만 허용한다면, 보안처리 후 HTTP 프로토콜로 변갱해줘야 하는데 이때 사용하는 게이트웨이이다.

서버의 앞 단에 위치하며, 보이지 않는 인터셉트 게이트웨이나 리버스 프락시 역할을 한다.

<aside>
💡 보안 처리가 되었다는 것이 HTTPS로 통신한다는 것을 의미하지 않는다. HTTP로 통신하더라도 사용자의 정보(세션, 쿠키, IP)등으로 보안을 처리할 수 있기 때문이다.

</aside>

# 8.3. 리소스 게이트웨이

게이트웨이의 일반적인 형태로 목적지 서버와 게이트웨이를 한 개의 서버로 결합한다.

![스크린샷 2023-05-26 오후 2.06.13.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/84069220-33aa-4ba5-be5f-604960005bd1/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.06.13.png)

위 그림은 두 클라이언트와 하나의ㅏ 게이트웨이 애플리케이션 서버가 연결된 것으로, 게이트웨이 앺ㄹ리케이션 서버는 두 클라이언트가 HTTP를 통해 서로 여춍하여 응답하는 중개인 역할을 한다. 요청이 들어오면 게이트웨이의 API를 통해 서버의 내부 프로그램을 돌리고 다시 HTTP로 응답하는 식이다.

애플리케이션 게이트웨이에서 유명했던 최초의 API는 공용 게이트웨이 인터페이스(CGI)로 최초의 서버 확장이자 지금까지도 가장 널리 쓰이는 서버 확장이다.

## 8.3.1. CGI(Common Gateway Interface)

웹에서 동적인 HTML, 신용 카드 처리, 데이터베이스 질의 등에 사용된다. 초기에는 매 CGI 요청마다 프로세스를 생성하기 때문에 부하가 컸지만 어느 정도 기술이 발전하면서 이런 성능 저하는 해결됐다.

## 8.3.2. 서버 확장 API

HTTP와 모듈을 직접 연결하는 인터페이스로, FPSE(FrontPage Server Extension)가 유명하고 클라이언트로 부터 RPC(Remote Procedure Call) 명령을 인식할 수 있다.

# 8.4. 애플리케이션 인터페이스와 웹 서비스

데이터를 교환할 때 두 어플리케이션의 프로토콜 인터페이스를 맞추는 것은 까다로운 이슈였다. HTTP 메시지 포맷으로는 제약이 있었기 때문이다. 여기서 HTTP 메시지에 XML을 사용하여 정보 교환을 하는 방식을 표준으로 삼았고 이를 SOAP(Simple Object Access Protocol)이라 한다.

# 8.5. 터널

웹 터널은 HTTP 프로토콜을 지원하지 않는 애플리케이션에 HTTP 애플리케이션을 사용해 접근하는 방법을 제공한다.

HTTP 커넥션을 통해서 HTTP이 아닌 트래픽을 전송할수 있으며, 다른 프로토콜을 HTTP 위에 올릴 수 있다.

## 8.5.1. CONNECT로 HTTP 터널 커넥션을 맺음

![스크린샷 2023-05-26 오후 2.39.47.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6005affd-c734-499f-a6bf-a1d62bef8492/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.39.47.png)

- (a): 클라이언트는 게이트 웨이에 터널을 연결하기 위해 요청을 보낸다.
- (b), (c): TCP 커넥션이 생성되었다.
- (d): TCP 커넥션이 맺어지면 게이트웨이는 클라이언트에 알린다.
- 그 후 터널이 연결되고 양방향으로 데이터를 주고 받는다. 클라이언트의 데이터는 TCP 커넥션으로 바로 전달되며, 서버로부터 전송된 데이터 역시 HTTP 터널을 통해 클라이언트에 전달된다.

CONNECT 요청을 한 뒤, 연결되면 그 때부터 양방향으로 데이터를 주고 받는다. CONNECT 요청과 응답은 조금 특징이 있다. 요청에는 CONNECT란 메서드를, 응답에는 Content-Type 헤더를 포함할 필요가 없다.

```tsx
CONNECT www.kakao.com:443 HTTP/1.1
User-agent: Mozilla/6.0
```

```tsx
HTTP/1.1 200 Connection Established
Proxy-agent: Netspace-Proxy/1.1
```

## 8.5.2. 데이터 터널링, 시간, 커넥션 관리

메시지를 더 빨리 보내기 위해 CONNECT 요청을 하고 응답을 받지 않은 상태로 데이터를 전송한다고 한다. 서버는 이런 동작을 이해하고 적절히 처리를 해줘야 한다. 만약 데이터 교환 중에 커넥션이 닫히면 데이터는 증발한다.

## 8.5.3. SSL 터널링

웹 터널은 원래 방화벽을 통해 암호화된 SSL 트래픽을 전달하기위해 개발 됐다.

![스크린샷 2023-05-26 오후 2.49.13.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2c676fcc-fa69-4e90-9067-2340ecab942e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.49.13.png)

![스크린샷 2023-05-26 오후 2.50.33.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b36fd4d6-3f01-457d-92cf-52ab58e73a06/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.50.33.png)

## 8.5.4. SSL 터널링 vs HTTP/HTTPS 게이트웨이

HTTPS 프로토콜(SSL 상의 HTTP)은 다른 프로토콜과 같은 방식으로 게이트웨이를 통과할 수 있다.

## 8.5.5. 터널 인증

HTTP의 기능을 터널과 함께 사용 가능한데, 프락시 인증 기능을 통해 클라이언트가 터널을 사용할 수 있는 권한이 있는지 터널에서 사용할 수 있다.

![게이트웨이는 터널 사용 하거를 내리기 전에 프락시 인증을 할 수 있다.](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8bae49be-d826-454b-a6d6-27cf7db73956/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.52.34.png)

게이트웨이는 터널 사용 하거를 내리기 전에 프락시 인증을 할 수 있다.

## 8.5.6. 터널 보안에 대한 고려사항들

터널의 오용을 최소화하기 위해, 게이트웨이는 HTTPS 전용 포트인 443 같이 잘 알려진 특정 포트만을 터널링할 수 있게 허용해야 한다.

# 8.6. 릴레이

HTTP 릴레이는 HTTP 명세를 완전히 준수하지 않는 간단한 HTTP 프락시이다. 커넥션을 맺기 위한 HTTP 통신을 한 다음에 HTTP 메시지가 아닌 바이트를 맹목적으로 전달한다. 이때 릴레이 중에는 멍청한 프락시 문제를 겪는 릴레이가 있다. keep-alive 커넥션 헤더를 이해하지 못해 행에 걸리는 것이다.

# 9. 웹로봇

웹 로봇의 예시

- 주식시장 서버에 매 분 HTTP GET 요청을 보내고, 여기서 얻은 데이터를 활용해 주가 추이 그래프를 생성하는 그래프 로봇
- 월드 와이드 웹의 규모와 진화에 대한 통계 정보를 수집하는 웹 통계 조사 로봇. 이것들은 웹을 떠돌면서 페이지의 개수를 세고, 각 페이지의 크기, 언어, 미디어 타입을 기록한다.
- 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇.
- 상품에 대한 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰의 가탈로그에서 웹페이지를 수집하는 가격 비교 로봇

# 9.1. 크롤러와 크롤링

루트로부터 시작해 링크된 페이지를 모두 가져와 재귀적으로 반복 순회하는 것을 크롤링이라 하고, 크롤링하는 로봇을 크롤러라 한다.

## 9.1.1. 어디에서 시작하는가: ‘루트 집합’

크롤러가 방문을 시작하는 URL의 초기 잡합을 루트 집이라 불린다.

![스크린샷 2023-05-26 오후 3.11.32.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/402081ca-f027-43bc-bee2-96c292537faf/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.11.32.png)

위 이미지에서 A, G, S 만 있다면 모든 페이지를 다 찾아볼수 있다.

일반적으로 www.google.com 같은 검색 엔진이 루트의 좋은 예다.

## 9.1.2. 링크 추출과 상대 링크 정상화

HTML은 여러 링크로 연결되어 있을 수 있다. HTML에는 상대 URL을 사용하는데 크롤러는 절대 URL로 바꿀 필요가 있다.

## 9.1.3. 순환 피하기

웹 크롤링을 할 때 순환에 빠질 위험이 있다. 자기 참조나 순환 참조가 얼마든지 가능하기 때문이다.

![스크린샷 2023-05-26 오후 3.18.24.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4dbfab1e-d6c0-4bb9-b265-adb60b979b9f/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.18.24.png)

이를 피하기 위해 어디를 방문했는지 기록해야 한다.

## 9.1.4. 루프와 중복

크롤러가 루프에 빠지만 아무일도 못한다. 그리고 같은 페이지를 계속 가져오게 되면 부담이 된다.(효율을 위해 메모리에 데이터를 저장하고 있다.)

## 9.1.5. 빵 부스러기의 흔적

크롤러가 크롤링할 웹 사이트의 개수는 매우 많고 endpoint 까지 따지면 훨씬 더 많아진다. 내가 이전에 방문했는지에 대한 여부를 아는 것도 매우 중요하고, 한 웹사이트의 엔드 포인트들을 구조적으로 관리하는 것도 의미가 있다. 그렇기 때문에 크롤러가 검색 트리나 해시 태이블, 느슨한 존재 비트맵 등을 사용한다.

또 로봇 프로그램이 갑작스럽게 중단된 경우를 대비해 체크 포인트(게임의 save포인트)를 사용하거나, 웹이 방대하기 때문에 여러 크롤러가 협업하여 부분적으로 말아서 크롤링하는 파티셔닝도 사용한다.

## 9.1.6. 별칭(alias)과 로봇 순환

URL이 별칭을 가질수 있는 이상, 어떤 페이지를 이 전에 방문했는지 판단하는게 쉽지 않을 수 있다.

- 기본 포트가 80번일 때
    - [http://www.foo.com:80/bar.html](http://www.foo.com/bar.html)
    - http://www.foo.com/bar.html
- 인코딩 문자열
    - http://www.foo.com/~fred
    - [http://www.foo.com/%7fred](http://www.foo.com/%7Ffred)
- 태그에 따라 페이지가 바뀌지 않을 때
    - [http://www.foo.com/x.html](http://www.foo.com/x.html#middle)
    - http://www.foo.com/x.html#middle
- 서버가 대소문자를 구분하지 않을 때
    - [http://www.foo.com/readme.html](http://www.foo.com/README.HTM)
    - http://www.foo.com/README.HTML
- 기본 페이지가 index.html 일 때
    - [http://www.foo.com/](http://www.foo.com/index.html)
    - http://www.foo.com/index.html
- 도메인과 IP 주소가 1:1 매칭될 때
    - [http://www.foo.com/index.html](http://209.231.87.45/index.html)
    - http://209.231.87.45/index.html

## 9.1.7. URL 정규화 하기

문자열의 내용은 다르지만, 결과적으로 같은 endpoint를 가리키는 경우를 피하고자 다음 규칙에 따라 정규화를 한다.

1. 포트 번호가 명시되어 있지 않는다면 ':80'을 추가한다.
2. 모든 %xx 이스케이핑된 문자들을 대응하는 원래 문자로 변환한다.
3. #태그들을 제거 한다.

## 9.1.8. 파일 시스템 링크 순환

![스크린샷 2023-05-26 오후 3.41.32.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/cff67752-193d-4cf7-b1c7-0d5be549ffb0/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.41.32.png)

파일 시스템의 심볼릭 링크가 끝 없는 디렉터리 계층을 만들 수 있다. (…/subdir/subdir/subdir/…)

## 9.1.9. 동적 가상 웹 공간

동적인 웹 공간에서 크롤러는 의미 없는 동작을 할 수 있다. 예를 들어 달력 페이지가 있을 때 다음 페이지를 누르면 링크로 이동이 될텐데 끝없는 페이지로 연결될 수 있다. 일반 사용자는 필요한 만큼 이동하겠지만, 크롤러는 계속된 링크 페이지로 이동할 수 있다.

## 9.1.10. 루프와 중복 피하기

크롤러는 효율적으로 크롤링하기 위해 몇가지 전략을 쓴다. 이 전략을 사용하면 일부의 손실이 있을 수 있지만 효율성을 위해 약간의 정확성은 희생한다.

- URL 정규화 - URL을 일정 형태로 만듦
- 너비 우선 크롤링 - 깊이 우선이 아닌 너비 우선 탐색
- 스로틀링 - 한 웹사이트에서 가져올 페이지 숫자를 제한
- URL 크기 제한 - URL 주소가 보통 1KB가 넘으면 크롤링을 거부 할 수 있다.
- URL/ 사이트 블랙리스트 - 여러 크롤러가 동시에 돌기 때문에 블랙리스트를 만들면 리소스 낭비를 줄인다.
- 패턴 발견 - '/sub/sub/sub/sub/sub/...' 같은 URL 패턴을 감지
- 콘텐츠 지문(fingerprint) - 문서에 대한 체크섬(MD5)을 사용한다.
- 사람의 모니터링 - 아직 까지는 사람이 모니터링을 해줘야 완성 된다.

# 9.2. 로봇의 HTTP

로봇은 아직 HTTP/1.0 버전을 사용한다. 이유는 HTTP/1.1 버전 이상 부터 헤더 구조도 크고 더 큰 메시지를 사용하는데 HTTP/1.0은 간소한 형태를 가지고 있다.

## 9.2.1. 요청 헤더 식별하기

로봇은 간소한 메시지를 사용하지만, 신원 증명을 해야 하기 때문에 다음의 헤더 값은 꼭 사용한다.

User-Agent - 서버에게 로봇인지 명확히 알린다.

From - 로봇의 사용자/관리자 이메일 주소를 제공한다.

Referer - 현재 요청 URL을 포함한 문서의 URL을 제공한다.

Accept - 로봇은 보통은 텍스트 또는 이미지만 받게 될 것이다.

## 9.2.2. 가상 호스팅

가상 호스팅을 사용하는 서버에게 정확히 요청을 보내려면 Host 헤더를 요청 메시지에 포함시켜야 한다.

그렇지 않으면 두 개의 사이트를 운영하는 서버에 요청을 보냈을 때 다른 URL의 값, 콘텐츠를 얻을 수 있다.

![스크린샷 2023-05-26 오후 4.05.32.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ecb5dd2e-cd7a-4e35-a1ad-500dcb9d7e81/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.05.32.png)

## 9.2.3. 조건부 요청

7장 캐시에서 다뤘던 조건부 요청을 사용하여, 데이터가 변경되지 않았다면 크롤러도 skip 하도록 해야한다.

## 9.2.4. 응답 다루기

로봇들도 HTTP 응답을 다룰 줄 알아야 한다. 상태 코드 뿐 아니라 본문에 있는 HTTP-EQUIV 값을 읽어 들여 특정한 동작도 이해할 수 있어야 한다.

## 9.2.5. User-Agent 타겟팅

웹 관리자들은 로봇들이 방문할 것을 알고 로봇들에게 맞는 응답을 줄 수 있도록 전략을 세워야 한다.

# 9.3. 부적절하게 동작하는 로봇들

로봇이 저지르는 실수들

- 폭주하는 로봇: 로봇은 사람보다 더 빠르게 HTTP 요청은 만들수 있고, 로봇이 논리적 에러를 갖고 있거나 순환에 빠졌다면 웹 서버에 극심한 부하를 안겨줄 수 있다.
- 오래되거나 사라진 URL
- 길고 잘못된 URL
- 호기심이 지나친 로봇: 웹 관리자가 알려지기 원하지 않을 경우
- 동적 게이트웨이 접근

# 9.4. 로봇 차단하기

robots.txt 파일을 사용한다.

## 9.4.1. 로봇 차단 표준

1996년에 만든 표준을 지금도 사용중이다. 주로 사용되는건 v1.0이다.

## 9.4.2. 웹 사이트와 robots.txt 파일들

웹 마스터가 하나의 robots.txt 파일을 만들 책임이 있다.

## 9.4.3. robots.txt 파일 포멧

```tsx
# this robots.txt file allows Slurp & Webcrawler to crawl
# the public parts of our site, but no other robots...

User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disallow:
```

1. **User-agent**: robots.txt 에서 지정하는 크롤링 규칙이 적용되어야 할 크롤러를 지정합니다.
2. **Allow**: 크롤링을 허용할 경로입니다 (/ 부터의 상대 경로).
3. **Disallow**: 크롤링을 제한할 경로입니다 (/ 부터의 상대 경로).
4. **Sitemap**: 사이트맵이 위치한 경로의 전체 URL입니다 (https:// 부터 /sitemap.xml 까지의 전체 절대경로 URL).

## 9.4.4. 그 외에 알아둘 점

## 9.4.5. robots.txt의 캐싱과 만료

필요에 따라 캐싱한다. 매번 가져오는 것도 비효율이고 크롤러가 HTTP/1.1도 아니기 때문에 캐시 지시자도 이해 못 할수도 있다. Cache-Control, Expires에 적당한 주기를 줘야한다(7일도 길다고 함)

## 9.4.6. 로봇 차단 펄 코드

## 9.4.7. HTML 로봇 제어 META 태그

```tsx
<meta name="robots" content="제어에 관련된 명령어">
```

HTML 문서 내 name 속성이 robots인 메타태그를 통해 로봇을 제어 할수 있다.

NOINDEX - 이 페이지를 처리 하지마라

NOFOLLOW - 이 페이지의 링크 페이지를 크롤링 하지마라

INDEX, FOLLOW 가 위에 반대되는 개념이고

NOARCHIVE - 이 페이지를 캐시하지 마라

ALL - INDEX + FOLLOW

NONE - NOINDEX + NOFOLLOW

### 검색 엔진 META 태그

로봇에 관련된 태그 외에도 아래 검색 엔진 메타태그는 색인을 만드는 검색엔진 로봇들에 대해 유용하다.

- description / 텍스트
    - 저자가 웹 페이지의 짧은 요약을 정의할 수 있게 한다.
- keywords / 쉼표 목록
    - 검색을 돕기 위한 단어
- revist-after / 숫자 days
    - 로봇 또는 검색 엔진에게 이 페이지는 쉽게 벼경되기 때문에 지정된 날짜가 지났을 때 다시 방문해야 한다고 지시

# 9.5. 로봇 에티켓

1993년 웹 로봇 커뮤니티 개척자 마틴 코스터의 가이드 라인

[The Web Robots Pages](https://www.robotstxt.org/)

# 9.6. 검색엔진

## 9.6.1. 넓게 생각하라.

웹 전체를 크롤링하는 것은 쉽지 않다. 복잡한 크롤러를 여러개 사용해야 한다.

## 9.6.2. 현대적인 검색엔진의 아키텍처

검색엔진은 풀 텍스트 색인(full-text indexes)이라고 하는 복잡한 로컬 데이터베이스를 생성하여 크롤링한 데이터를 쌓는다. 하지만 웹 페이지는 계속 변화하기 때문에 특정 순간의 스냅샷에 불과하다.

![스크린샷 2023-05-26 오후 4.41.17.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/81463c87-0d3e-4af1-9719-ebcd77ba9664/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.41.17.png)

## 9.6.3. 풀 텍스트 색인(full-text indexes)

단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 알려줄 수 있는 데이터 베이스

![스크린샷 2023-05-26 오후 4.42.07.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1d4b1d65-569a-41a9-8978-96bceeefe9a5/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.42.07.png)

## 9.6.4. 질의 보내기

검색 엔진에서 검색하면 웹서버는 검색 게이트웨이 애플리케이션(검색 엔진 데이터베이스)에게 넘겨주고 게이트 웨이는 문서의 목록을 결과로 준다. 여기서 게이트웨이 프로그램은 검색 질의를 추출하고 웹 UI 질의를 풀 텍스트 색인을 검색할 때 사용하는 표현식으로 변환한다.

웹 서버는 그것을 이용하여 사용자를 위한 HTML 페이지로 변환한다.

![스크린샷 2023-05-26 오후 4.45.35.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6d6a1fc3-bf72-4303-bda3-aef7717d9753/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.45.35.png)

## 9.6.4. 검색 결과를 정렬하고 보여주기

검색 결과는 적으면 몇개 많으면 수천, 수만 개가 보일 수 있다. 사용자가 검색한 결과에 가장 적절한 결과를 보여줘야 하기 때문에 크롤러가 해당 페이지에 대한 설명을 정확히 이해하기 위해 자세히 적어야 한다.

## 9.6.5. 스푸핑

가짜 페이지가 크롤러를 더 잘 속이고, 이에 따라 크롤러도 진화하는 끊없는 싸움을 스푸핑이라 한다.